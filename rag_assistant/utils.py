import logging
logging.getLogger("pdfminer").setLevel(logging.ERROR)

import os
import time
import shutil
import pdfplumber
from tqdm import tqdm
from retrying import retry
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from pdfminer.pdfparser import PDFSyntaxError
from rag_assistant import config

def parse_pdfs(pdf_dir: str) -> list[Document]:
    docs: list[Document] = []
    all_files = os.listdir(pdf_dir)
    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]

    print(f"[INFO] –í—Å–µ–≥–æ PDF-—Ñ–∞–π–ª–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(pdf_files)}")

    for fname in pdf_files:
        path = os.path.join(pdf_dir, fname)
        try:
            print(f"[LOAD] –ó–∞–≥—Ä—É–∂–∞–µ–º PDF: {fname}")
            with pdfplumber.open(path) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 20:
                        print(f"[WARN] –ü—É—Å—Ç–∞—è –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1} –≤ {fname}")
                        continue
                    meta = {'source': fname, 'page': i + 1}
                    docs.append(Document(page_content=text, metadata=meta))
        except PDFSyntaxError:
            print(f"[ERROR] –ü–æ–≤—Ä–µ–∂–¥—ë–Ω PDF: {fname}")
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Å {fname}: {e}")

    print(f"[RESULT] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ {len(pdf_files)} PDF-—Ñ–∞–π–ª–æ–≤")
    return docs


@retry(stop_max_attempt_number=3, wait_fixed=1000)
def get_or_create_vectorstore_incremental(pdf_dir, persist_dir):
    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002",
        batch_size=64,
        request_timeout=60,
        show_progress_bar=True
    )

    os.makedirs(persist_dir, exist_ok=True)

    vectordb = Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings
    )

    existing_files = set()
    try:
        existing_metadatas = vectordb.get()["metadatas"]
        for m in existing_metadatas:
            if isinstance(m, dict) and "source" in m:
                existing_files.add(m["source"])
    except Exception as e:
        print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")

    print(f"[INFO] –í –±–∞–∑–µ —É–∂–µ –µ—Å—Ç—å {len(existing_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")

    new_pdfs = [
        f for f in os.listdir(pdf_dir)
        if f.lower().endswith(".pdf") and f not in existing_files
    ]

    if not new_pdfs:
        print("[INFO] –ù–æ–≤—ã—Ö PDF-—Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ë–∞–∑–∞ –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
        return vectordb

    print(f"[INFO] –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö PDF: {len(new_pdfs)}")

    docs: list[Document] = []
    for fname in new_pdfs:
        try:
            path = os.path.join(pdf_dir, fname)
            with pdfplumber.open(path) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 20:
                        continue
                    meta = {'source': fname, 'page': i + 1}
                    docs.append(Document(page_content=text, metadata=meta))
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {fname}: {e}")

    print(f"[INFO] –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {len(docs)}")

    if not docs:
        print("[WARN] –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è.")
        return vectordb

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    print(f"[INFO] –ß–∞–Ω–∫–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {len(chunks)}")

    batch_size = 64
    for i in tqdm(range(0, len(chunks), batch_size), desc="üì• –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤"):
        batch = chunks[i:i + batch_size]
        try:
            vectordb.add_documents(batch)
            vectordb.persist()
            time.sleep(0.5)
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {i // batch_size + 1}: {e}")

    print("[SUCCESS] –ù–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –±–∞–∑—É.")
    return vectordb
